{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f0f6639",
   "metadata": {},
   "source": [
    "Cell 1 – Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2351b2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, random, re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from transformers import pipeline\n",
    "\n",
    "# Basic scraper settings\n",
    "BASE_HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "        \"(KHTML, like Gecko) Chrome/126.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Categories from Wadhefa\n",
    "CATEGORIES = {\n",
    "    \"IT\": \"https://www.wadhefa.com/jobfind.php?action=search&jids%5B%5D=75&jids%5B%5D=76&jids%5B%5D=84\",\n",
    "    \"Marketing\": \"https://www.wadhefa.com/jobfind.php?action=search&jids%5B%5D=72&jids%5B%5D=73&jids%5B%5D=74\",\n",
    "    \"Finance\": \"https://www.wadhefa.com/jobfind.php?action=search&jids%5B%5D=64&jids%5B%5D=77&jids%5B%5D=79\",\n",
    "    \"Engineering\": \"https://www.wadhefa.com/jobfind.php?action=search&jids%5B%5D=80&jids%5B%5D=81&jids%5B%5D=82\",\n",
    "    \"Healthcare\": \"https://www.wadhefa.com/jobfind.php?action=search&jids%5B%5D=96&jids%5B%5D=97\",\n",
    "    \"Education\": \"https://www.wadhefa.com/jobfind.php?action=search&jids%5B%5D=67\"\n",
    "}\n",
    "\n",
    "LINKS_PER_CATEGORY = 50  # limit for quick testing (increase later)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c168836d",
   "metadata": {},
   "source": [
    "Cell 2 – Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7ea31e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=8))\n",
    "def fetch(url: str) -> str:\n",
    "    resp = requests.get(url, headers=BASE_HEADERS, timeout=25)\n",
    "    resp.raise_for_status()\n",
    "    return resp.text\n",
    "\n",
    "def extract_links_from_list_page(list_url: str, limit: int = 50):\n",
    "    \"\"\"Extract job links from a Wadhefa listing page.\"\"\"\n",
    "    html = fetch(list_url)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    links = []\n",
    "    for a in soup.select('a.tablelist[href*=\"/details/job/\"]'):\n",
    "        href = a.get(\"href\")\n",
    "        if href:\n",
    "            links.append(urljoin(list_url, href))\n",
    "        if len(links) >= limit:\n",
    "            break\n",
    "    return list(dict.fromkeys(links))  # remove duplicates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257c4093",
   "metadata": {},
   "source": [
    "Cell 3 – Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "709aa9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "AR_DIACRITICS_RE = re.compile(r\"[\\u064B-\\u0652]\")\n",
    "\n",
    "def normalize_ar_keep_ascii(text: str) -> str:\n",
    "    \"\"\"Light cleanup for Arabic text (keep ASCII like emails/numbers).\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    t = text.replace(\"\\u00A0\",\" \")\n",
    "    t = re.sub(r\"\\u0640\", \"\", t)     # tatweel\n",
    "    t = AR_DIACRITICS_RE.sub(\"\", t)  # diacritics\n",
    "    t = t.replace(\"أ\",\"ا\").replace(\"إ\",\"ا\").replace(\"آ\",\"ا\") \\\n",
    "         .replace(\"ى\",\"ي\").replace(\"ؤ\",\"و\").replace(\"ئ\",\"ي\")\n",
    "    t = re.sub(r\"[ \\t]+\", \" \", t).strip()\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be23e13c",
   "metadata": {},
   "source": [
    "Cell 4 – Job Detail Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "588b0562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_job_detail(job_url: str, category_label: str):\n",
    "    html = fetch(job_url)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # description\n",
    "    d_el = soup.select_one(\"td.td4textarea\")\n",
    "    desc_raw = d_el.get_text(\"\\n\", strip=True) if d_el else \"\"\n",
    "    description = normalize_ar_keep_ascii(desc_raw)\n",
    "\n",
    "    return {\n",
    "        \"category\": category_label,\n",
    "        \"description\": excel_safe(description),\n",
    "        \"url\": job_url,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e07da30",
   "metadata": {},
   "source": [
    "Cell 5 – Track Seen Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ab7916c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_seen_jobs(path=\"seen_jobs.txt\"):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return set(f.read().splitlines())\n",
    "    except FileNotFoundError:\n",
    "        return set()\n",
    "\n",
    "def save_seen_jobs(seen_jobs, path=\"seen_jobs.txt\"):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for url in seen_jobs:\n",
    "            f.write(url + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "97ca780c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maanx\\Documents\\VS code\\samsung project\\job_posts_part\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a445c82",
   "metadata": {},
   "source": [
    "Cell 6 – Load Job Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "635d7d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.9578908085823059}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "model_path = \"./job_classifier\"   # since you're already inside job_posts_part\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "job_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    function_to_apply=\"softmax\",\n",
    "    truncation=True,   # 🔹 cut text if too long\n",
    "    max_length=512     # 🔹 AraBERT max length\n",
    ")\n",
    "\n",
    "\n",
    "# quick test\n",
    "test_text = \"مطلوب مطور بايثون للعمل على تطبيقات ويب\"\n",
    "print(job_classifier(test_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "10b443d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job classifier label mapping: {0: 'IT', 1: 'Marketing', 2: 'Finance', 3: 'Engineering', 4: 'Healthcare', 5: 'Education'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the training dataset categories (used when training the job classifier)\n",
    "df_jobs = pd.read_csv(\"wadhefa_dataset.csv\")  \n",
    "\n",
    "# Build label mappings\n",
    "label_list = df_jobs['category'].unique().tolist()\n",
    "label_to_id = {label: idx for idx, label in enumerate(label_list)}\n",
    "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "\n",
    "print(\"Job classifier label mapping:\", id_to_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3cb93a",
   "metadata": {},
   "source": [
    "Cell 7 – Classification Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "270e142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_job(description: str) -> str:\n",
    "    if not description or not description.strip():\n",
    "        return \"Unknown\"\n",
    "\n",
    "    result = job_classifier(description)[0]  # e.g., {'label': 'LABEL_0', 'score': 0.97}\n",
    "\n",
    "    if result[\"label\"].startswith(\"LABEL_\"):\n",
    "        label_idx = int(result[\"label\"].split(\"_\")[-1])\n",
    "        return id_to_label.get(label_idx, \"Unknown\")\n",
    "\n",
    "    if result[\"label\"] in id_to_label.values():\n",
    "        return result[\"label\"]\n",
    "\n",
    "    return \"Unknown\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f7f21",
   "metadata": {},
   "source": [
    "Cell 8 – Main Scraper + Classifier Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3cf25b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping category: IT\n",
      "  Found 0 new jobs\n",
      "Scraping category: Marketing\n",
      "  Found 0 new jobs\n",
      "Scraping category: Finance\n",
      "  Found 0 new jobs\n",
      "Scraping category: Engineering\n",
      "  Found 0 new jobs\n",
      "Scraping category: Healthcare\n",
      "  Found 0 new jobs\n",
      "Scraping category: Education\n",
      "  Found 0 new jobs\n"
     ]
    }
   ],
   "source": [
    "all_rows = []\n",
    "seen = load_seen_jobs()\n",
    "\n",
    "for label, list_url in CATEGORIES.items():\n",
    "    print(f\"Scraping category: {label}\")\n",
    "    links = extract_links_from_list_page(list_url, limit=LINKS_PER_CATEGORY)\n",
    "    new_links = [u for u in links if u not in seen]\n",
    "\n",
    "    print(f\"  Found {len(new_links)} new jobs\")\n",
    "    for u in new_links:\n",
    "        try:\n",
    "            row = parse_job_detail(u, category_label=label)\n",
    "            row[\"predicted_category\"] = classify_job(row[\"description\"])  # 🔹 classify immediately\n",
    "            all_rows.append(row)\n",
    "            seen.add(u)\n",
    "            time.sleep(random.uniform(0.6, 1.2))\n",
    "        except Exception as e:\n",
    "            print(\"  Failed:\", u, e)\n",
    "\n",
    "save_seen_jobs(seen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3cd154",
   "metadata": {},
   "source": [
    "Cell 9 – Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3c80b20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new jobs found.\n"
     ]
    }
   ],
   "source": [
    "if all_rows:\n",
    "    new_df = pd.DataFrame(all_rows).drop_duplicates()\n",
    "\n",
    "    # Save new jobs only\n",
    "    new_df.to_csv(\"new_classified_jobs.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Append/merge into full dataset\n",
    "    if os.path.exists(\"classified_wadhefa_dataset.csv\"):\n",
    "        full_df = pd.concat([pd.read_csv(\"classified_wadhefa_dataset.csv\"), new_df], ignore_index=True).drop_duplicates()\n",
    "    else:\n",
    "        full_df = new_df\n",
    "\n",
    "    full_df.to_csv(\"classified_wadhefa_dataset.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"✅ Saved updated dataset with classified jobs\")\n",
    "else:\n",
    "    print(\"No new jobs found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "21e6c96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample job description:\n",
      " - تحديد مواصفات الحلول البديلة والاضافات والتحديثات اللازمة ويدرس جدواها فنيا واقتصاديا\n",
      "- مراقبة الاستخدام والابلاغ عن المشكلات والاخطاء والعمل علي حلها والتاكد من الاستخدام السليم\n",
      "- تنفيذ احدث التطبيقات والنظم في مجال تقنية وامن المعلومات وجدولة دورية لصيانة نظم المعلومات ووضع برامج الدعم والصيانة\n",
      "- تنسيق التعاقدات التقنية وعمليات الشراء الخاصة بها\n",
      "- القيام بعمليات تحديد احتياجات المستخدمين في مختلف الوحدات التنظيمية في الجهة والقيام بمراجعتها وتقديم الاقتراحات المعنية بتطوير ووضع المواصفات التقنية للانظمة والتطبيقات اللازمة والقيام بعملها\n",
      "- اعداد برامج وخطط العمل المتعلقة باعمال تقنية المعلومات وعمليات تصميم وتطوير البرامج والتطبيقات والقيام بها وعمل انشطة دعم المستخدم بهدف توفير الحلول التقنية اللازمة عند مواجهة اية صعوبات او مشاكل\n",
      "- متابعة اخر المستجدات والتطورات المتعلقة بشوون وعمليات تقنية المعلومات والتوجيه بتبني المناسب منها، وتحديد وضمان توافر جميع الموارد والمواد اللازمة لتنفيذ الاعمال والانشطة التخصصية باحدث الاساليب والممارسات ذات العلاقة\n",
      "- القيام بعمليات التطوير وتطبيق المعايير اللازمة لضمان امن نظم المعلومات بما يشمل التاكد من هوية المستخدمين وصلاحياتهم، والتعامل مع المخاطر والتهديدات المتوقعة قبل حدوثها، ومراقبة سرية ونقل البيانات عبر الشبكة وضمان توافر الخدمات وعدم انقطاعها\n",
      "- عمل كافة العمليات اليومية لاعمال تقنية المعلومات وضمان وجود منهجيات فعالة للبحث عن البرامج والتطبيقات التي تلبي احتياجات المستخدم النهايي في الجهة وضمان امنها وملاءمتها لمختلف المتطلبات والمواصفات والتقنيات الحالية\n",
      "- دعم تنفيذ وتطوير نظام ادارة المعرفة ISO 30401 من خلال توفير البيية التقنية اللازمة لتوثيق وتخزين ومشاركة المعرفة الموسسية، وضمان تكامل الانظمة والتطبيقات المستخدمة في ادارة المعرفة، وتحقيق امن وسلامة المحتوي المعرفي الالكتروني \n",
      "\n",
      "Raw classifier output: [{'label': 'LABEL_0', 'score': 0.9728232026100159}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the classified jobs dataset\n",
    "df = pd.read_csv(\"classified_wadhefa_dataset.csv\")\n",
    "\n",
    "# Take the first job description\n",
    "sample_text = df[\"description\"].iloc[0]\n",
    "print(\"Sample job description:\\n\", sample_text, \"\\n\")\n",
    "\n",
    "# Run it through the classifier\n",
    "result = job_classifier(sample_text)\n",
    "print(\"Raw classifier output:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a046df74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved. New columns: ['category', 'description', 'url', 'predicted_category']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"classified_wadhefa_dataset.csv\")\n",
    "\n",
    "# Drop the title column if it exists\n",
    "if \"title\" in df.columns:\n",
    "    df = df.drop(columns=[\"title\"])\n",
    "\n",
    "# Save cleaned dataset\n",
    "df.to_csv(\"classified_wadhefa_dataset.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Cleaned dataset saved. New columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "26eaf267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved. Remaining rows: 3\n",
      "predicted_category\n",
      "IT           2\n",
      "Marketing    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file\n",
    "df = pd.read_csv(\"classified_wadhefa_dataset.csv\")\n",
    "\n",
    "# Keep only rows where predicted_category != \"Unknown\"\n",
    "df_clean = df[df[\"predicted_category\"] != \"Unknown\"]\n",
    "\n",
    "# Save back\n",
    "df_clean.to_csv(\"classified_wadhefa_dataset.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Cleaned dataset saved. Remaining rows:\", df_clean.shape[0])\n",
    "print(df_clean[\"predicted_category\"].value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
