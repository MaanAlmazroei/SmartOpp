{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f0f6639",
   "metadata": {},
   "source": [
    "Cell 1 â€“ Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2351b2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, random, re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from transformers import pipeline\n",
    "\n",
    "# Basic scraper settings\n",
    "BASE_HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "        \"(KHTML, like Gecko) Chrome/126.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Categories from Wadhefa\n",
    "CATEGORIES = {\n",
    "    \"IT\": \"https://www.wadhefa.com/jobfind.php?action=search&jids%5B%5D=75&jids%5B%5D=76&jids%5B%5D=84\",\n",
    "    \"Marketing\": \"https://www.wadhefa.com/jobfind.php?action=search&jids%5B%5D=72&jids%5B%5D=73&jids%5B%5D=74\",\n",
    "    \"Finance\": \"https://www.wadhefa.com/jobfind.php?action=search&jids%5B%5D=64&jids%5B%5D=77&jids%5B%5D=79\",\n",
    "    \"Engineering\": \"https://www.wadhefa.com/jobfind.php?action=search&jids%5B%5D=80&jids%5B%5D=81&jids%5B%5D=82\",\n",
    "    \"Healthcare\": \"https://www.wadhefa.com/jobfind.php?action=search&jids%5B%5D=96&jids%5B%5D=97\",\n",
    "    \"Education\": \"https://www.wadhefa.com/jobfind.php?action=search&jids%5B%5D=67\"\n",
    "}\n",
    "\n",
    "LINKS_PER_CATEGORY = 50  # limit for quick testing (increase later)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c168836d",
   "metadata": {},
   "source": [
    "Cell 2 â€“ Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7ea31e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=8))\n",
    "def fetch(url: str) -> str:\n",
    "    resp = requests.get(url, headers=BASE_HEADERS, timeout=25)\n",
    "    resp.raise_for_status()\n",
    "    return resp.text\n",
    "\n",
    "def extract_links_from_list_page(list_url: str, limit: int = 50):\n",
    "    \"\"\"Extract job links from a Wadhefa listing page.\"\"\"\n",
    "    html = fetch(list_url)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    links = []\n",
    "    for a in soup.select('a.tablelist[href*=\"/details/job/\"]'):\n",
    "        href = a.get(\"href\")\n",
    "        if href:\n",
    "            links.append(urljoin(list_url, href))\n",
    "        if len(links) >= limit:\n",
    "            break\n",
    "    return list(dict.fromkeys(links))  # remove duplicates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257c4093",
   "metadata": {},
   "source": [
    "Cell 3 â€“ Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "709aa9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "AR_DIACRITICS_RE = re.compile(r\"[\\u064B-\\u0652]\")\n",
    "\n",
    "def normalize_ar_keep_ascii(text: str) -> str:\n",
    "    \"\"\"Light cleanup for Arabic text (keep ASCII like emails/numbers).\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    t = text.replace(\"\\u00A0\",\" \")\n",
    "    t = re.sub(r\"\\u0640\", \"\", t)     # tatweel\n",
    "    t = AR_DIACRITICS_RE.sub(\"\", t)  # diacritics\n",
    "    t = t.replace(\"Ø£\",\"Ø§\").replace(\"Ø¥\",\"Ø§\").replace(\"Ø¢\",\"Ø§\") \\\n",
    "         .replace(\"Ù‰\",\"ÙŠ\").replace(\"Ø¤\",\"Ùˆ\").replace(\"Ø¦\",\"ÙŠ\")\n",
    "    t = re.sub(r\"[ \\t]+\", \" \", t).strip()\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be23e13c",
   "metadata": {},
   "source": [
    "Cell 4 â€“ Job Detail Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "588b0562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_job_detail(job_url: str, category_label: str):\n",
    "    html = fetch(job_url)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # description\n",
    "    d_el = soup.select_one(\"td.td4textarea\")\n",
    "    desc_raw = d_el.get_text(\"\\n\", strip=True) if d_el else \"\"\n",
    "    description = normalize_ar_keep_ascii(desc_raw)\n",
    "\n",
    "    return {\n",
    "        \"category\": category_label,\n",
    "        \"description\": excel_safe(description),\n",
    "        \"url\": job_url,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e07da30",
   "metadata": {},
   "source": [
    "Cell 5 â€“ Track Seen Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ab7916c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_seen_jobs(path=\"seen_jobs.txt\"):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return set(f.read().splitlines())\n",
    "    except FileNotFoundError:\n",
    "        return set()\n",
    "\n",
    "def save_seen_jobs(seen_jobs, path=\"seen_jobs.txt\"):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for url in seen_jobs:\n",
    "            f.write(url + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "97ca780c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maanx\\Documents\\VS code\\samsung project\\job_posts_part\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a445c82",
   "metadata": {},
   "source": [
    "Cell 6 â€“ Load Job Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "635d7d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.9578908085823059}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "model_path = \"./job_classifier\"   # since you're already inside job_posts_part\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "job_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    function_to_apply=\"softmax\",\n",
    "    truncation=True,   # ğŸ”¹ cut text if too long\n",
    "    max_length=512     # ğŸ”¹ AraBERT max length\n",
    ")\n",
    "\n",
    "\n",
    "# quick test\n",
    "test_text = \"Ù…Ø·Ù„ÙˆØ¨ Ù…Ø·ÙˆØ± Ø¨Ø§ÙŠØ«ÙˆÙ† Ù„Ù„Ø¹Ù…Ù„ Ø¹Ù„Ù‰ ØªØ·Ø¨ÙŠÙ‚Ø§Øª ÙˆÙŠØ¨\"\n",
    "print(job_classifier(test_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "10b443d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job classifier label mapping: {0: 'IT', 1: 'Marketing', 2: 'Finance', 3: 'Engineering', 4: 'Healthcare', 5: 'Education'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the training dataset categories (used when training the job classifier)\n",
    "df_jobs = pd.read_csv(\"wadhefa_dataset.csv\")  \n",
    "\n",
    "# Build label mappings\n",
    "label_list = df_jobs['category'].unique().tolist()\n",
    "label_to_id = {label: idx for idx, label in enumerate(label_list)}\n",
    "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "\n",
    "print(\"Job classifier label mapping:\", id_to_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3cb93a",
   "metadata": {},
   "source": [
    "Cell 7 â€“ Classification Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "270e142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_job(description: str) -> str:\n",
    "    if not description or not description.strip():\n",
    "        return \"Unknown\"\n",
    "\n",
    "    result = job_classifier(description)[0]  # e.g., {'label': 'LABEL_0', 'score': 0.97}\n",
    "\n",
    "    if result[\"label\"].startswith(\"LABEL_\"):\n",
    "        label_idx = int(result[\"label\"].split(\"_\")[-1])\n",
    "        return id_to_label.get(label_idx, \"Unknown\")\n",
    "\n",
    "    if result[\"label\"] in id_to_label.values():\n",
    "        return result[\"label\"]\n",
    "\n",
    "    return \"Unknown\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f7f21",
   "metadata": {},
   "source": [
    "Cell 8 â€“ Main Scraper + Classifier Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3cf25b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping category: IT\n",
      "  Found 0 new jobs\n",
      "Scraping category: Marketing\n",
      "  Found 0 new jobs\n",
      "Scraping category: Finance\n",
      "  Found 0 new jobs\n",
      "Scraping category: Engineering\n",
      "  Found 0 new jobs\n",
      "Scraping category: Healthcare\n",
      "  Found 0 new jobs\n",
      "Scraping category: Education\n",
      "  Found 0 new jobs\n"
     ]
    }
   ],
   "source": [
    "all_rows = []\n",
    "seen = load_seen_jobs()\n",
    "\n",
    "for label, list_url in CATEGORIES.items():\n",
    "    print(f\"Scraping category: {label}\")\n",
    "    links = extract_links_from_list_page(list_url, limit=LINKS_PER_CATEGORY)\n",
    "    new_links = [u for u in links if u not in seen]\n",
    "\n",
    "    print(f\"  Found {len(new_links)} new jobs\")\n",
    "    for u in new_links:\n",
    "        try:\n",
    "            row = parse_job_detail(u, category_label=label)\n",
    "            row[\"predicted_category\"] = classify_job(row[\"description\"])  # ğŸ”¹ classify immediately\n",
    "            all_rows.append(row)\n",
    "            seen.add(u)\n",
    "            time.sleep(random.uniform(0.6, 1.2))\n",
    "        except Exception as e:\n",
    "            print(\"  Failed:\", u, e)\n",
    "\n",
    "save_seen_jobs(seen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3cd154",
   "metadata": {},
   "source": [
    "Cell 9 â€“ Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3c80b20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new jobs found.\n"
     ]
    }
   ],
   "source": [
    "if all_rows:\n",
    "    new_df = pd.DataFrame(all_rows).drop_duplicates()\n",
    "\n",
    "    # Save new jobs only\n",
    "    new_df.to_csv(\"new_classified_jobs.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Append/merge into full dataset\n",
    "    if os.path.exists(\"classified_wadhefa_dataset.csv\"):\n",
    "        full_df = pd.concat([pd.read_csv(\"classified_wadhefa_dataset.csv\"), new_df], ignore_index=True).drop_duplicates()\n",
    "    else:\n",
    "        full_df = new_df\n",
    "\n",
    "    full_df.to_csv(\"classified_wadhefa_dataset.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"âœ… Saved updated dataset with classified jobs\")\n",
    "else:\n",
    "    print(\"No new jobs found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "21e6c96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample job description:\n",
      " - ØªØ­Ø¯ÙŠØ¯ Ù…ÙˆØ§ØµÙØ§Øª Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ø¨Ø¯ÙŠÙ„Ø© ÙˆØ§Ù„Ø§Ø¶Ø§ÙØ§Øª ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© ÙˆÙŠØ¯Ø±Ø³ Ø¬Ø¯ÙˆØ§Ù‡Ø§ ÙÙ†ÙŠØ§ ÙˆØ§Ù‚ØªØµØ§Ø¯ÙŠØ§\n",
      "- Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ§Ù„Ø§Ø¨Ù„Ø§Øº Ø¹Ù† Ø§Ù„Ù…Ø´ÙƒÙ„Ø§Øª ÙˆØ§Ù„Ø§Ø®Ø·Ø§Ø¡ ÙˆØ§Ù„Ø¹Ù…Ù„ Ø¹Ù„ÙŠ Ø­Ù„Ù‡Ø§ ÙˆØ§Ù„ØªØ§ÙƒØ¯ Ù…Ù† Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³Ù„ÙŠÙ…\n",
      "- ØªÙ†ÙÙŠØ° Ø§Ø­Ø¯Ø« Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª ÙˆØ§Ù„Ù†Ø¸Ù… ÙÙŠ Ù…Ø¬Ø§Ù„ ØªÙ‚Ù†ÙŠØ© ÙˆØ§Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆØ¬Ø¯ÙˆÙ„Ø© Ø¯ÙˆØ±ÙŠØ© Ù„ØµÙŠØ§Ù†Ø© Ù†Ø¸Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆÙˆØ¶Ø¹ Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ø¯Ø¹Ù… ÙˆØ§Ù„ØµÙŠØ§Ù†Ø©\n",
      "- ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ¹Ø§Ù‚Ø¯Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ© ÙˆØ¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø´Ø±Ø§Ø¡ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù‡Ø§\n",
      "- Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø¹Ù…Ù„ÙŠØ§Øª ØªØ­Ø¯ÙŠØ¯ Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙÙŠ Ù…Ø®ØªÙ„Ù Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„ØªÙ†Ø¸ÙŠÙ…ÙŠØ© ÙÙŠ Ø§Ù„Ø¬Ù‡Ø© ÙˆØ§Ù„Ù‚ÙŠØ§Ù… Ø¨Ù…Ø±Ø§Ø¬Ø¹ØªÙ‡Ø§ ÙˆØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø§Ù„Ù…Ø¹Ù†ÙŠØ© Ø¨ØªØ·ÙˆÙŠØ± ÙˆÙˆØ¶Ø¹ Ø§Ù„Ù…ÙˆØ§ØµÙØ§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ù„Ù„Ø§Ù†Ø¸Ù…Ø© ÙˆØ§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© ÙˆØ§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø¹Ù…Ù„Ù‡Ø§\n",
      "- Ø§Ø¹Ø¯Ø§Ø¯ Ø¨Ø±Ø§Ù…Ø¬ ÙˆØ®Ø·Ø· Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ø¹Ù…Ø§Ù„ ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆØ¹Ù…Ù„ÙŠØ§Øª ØªØµÙ…ÙŠÙ… ÙˆØªØ·ÙˆÙŠØ± Ø§Ù„Ø¨Ø±Ø§Ù…Ø¬ ÙˆØ§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª ÙˆØ§Ù„Ù‚ÙŠØ§Ù… Ø¨Ù‡Ø§ ÙˆØ¹Ù…Ù„ Ø§Ù†Ø´Ø·Ø© Ø¯Ø¹Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ù‡Ø¯Ù ØªÙˆÙÙŠØ± Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ø¹Ù†Ø¯ Ù…ÙˆØ§Ø¬Ù‡Ø© Ø§ÙŠØ© ØµØ¹ÙˆØ¨Ø§Øª Ø§Ùˆ Ù…Ø´Ø§ÙƒÙ„\n",
      "- Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ø®Ø± Ø§Ù„Ù…Ø³ØªØ¬Ø¯Ø§Øª ÙˆØ§Ù„ØªØ·ÙˆØ±Ø§Øª Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø´ÙˆÙˆÙ† ÙˆØ¹Ù…Ù„ÙŠØ§Øª ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆØ§Ù„ØªÙˆØ¬ÙŠÙ‡ Ø¨ØªØ¨Ù†ÙŠ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ù…Ù†Ù‡Ø§ØŒ ÙˆØªØ­Ø¯ÙŠØ¯ ÙˆØ¶Ù…Ø§Ù† ØªÙˆØ§ÙØ± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ ÙˆØ§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø¹Ù…Ø§Ù„ ÙˆØ§Ù„Ø§Ù†Ø´Ø·Ø© Ø§Ù„ØªØ®ØµØµÙŠØ© Ø¨Ø§Ø­Ø¯Ø« Ø§Ù„Ø§Ø³Ø§Ù„ÙŠØ¨ ÙˆØ§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª Ø°Ø§Øª Ø§Ù„Ø¹Ù„Ø§Ù‚Ø©\n",
      "- Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªØ·ÙˆÙŠØ± ÙˆØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„Ø¶Ù…Ø§Ù† Ø§Ù…Ù† Ù†Ø¸Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¨Ù…Ø§ ÙŠØ´Ù…Ù„ Ø§Ù„ØªØ§ÙƒØ¯ Ù…Ù† Ù‡ÙˆÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙˆØµÙ„Ø§Ø­ÙŠØ§ØªÙ‡Ù…ØŒ ÙˆØ§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø®Ø§Ø·Ø± ÙˆØ§Ù„ØªÙ‡Ø¯ÙŠØ¯Ø§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© Ù‚Ø¨Ù„ Ø­Ø¯ÙˆØ«Ù‡Ø§ØŒ ÙˆÙ…Ø±Ø§Ù‚Ø¨Ø© Ø³Ø±ÙŠØ© ÙˆÙ†Ù‚Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ø¨Ø± Ø§Ù„Ø´Ø¨ÙƒØ© ÙˆØ¶Ù…Ø§Ù† ØªÙˆØ§ÙØ± Ø§Ù„Ø®Ø¯Ù…Ø§Øª ÙˆØ¹Ø¯Ù… Ø§Ù†Ù‚Ø·Ø§Ø¹Ù‡Ø§\n",
      "- Ø¹Ù…Ù„ ÙƒØ§ÙØ© Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ÙŠÙˆÙ…ÙŠØ© Ù„Ø§Ø¹Ù…Ø§Ù„ ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆØ¶Ù…Ø§Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù†Ù‡Ø¬ÙŠØ§Øª ÙØ¹Ø§Ù„Ø© Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¨Ø±Ø§Ù…Ø¬ ÙˆØ§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ØªÙŠ ØªÙ„Ø¨ÙŠ Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Ù‡Ø§ÙŠÙŠ ÙÙŠ Ø§Ù„Ø¬Ù‡Ø© ÙˆØ¶Ù…Ø§Ù† Ø§Ù…Ù†Ù‡Ø§ ÙˆÙ…Ù„Ø§Ø¡Ù…ØªÙ‡Ø§ Ù„Ù…Ø®ØªÙ„Ù Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª ÙˆØ§Ù„Ù…ÙˆØ§ØµÙØ§Øª ÙˆØ§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©\n",
      "- Ø¯Ø¹Ù… ØªÙ†ÙÙŠØ° ÙˆØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ø§Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© ISO 30401 Ù…Ù† Ø®Ù„Ø§Ù„ ØªÙˆÙÙŠØ± Ø§Ù„Ø¨ÙŠÙŠØ© Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„ØªÙˆØ«ÙŠÙ‚ ÙˆØªØ®Ø²ÙŠÙ† ÙˆÙ…Ø´Ø§Ø±ÙƒØ© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…ÙˆØ³Ø³ÙŠØ©ØŒ ÙˆØ¶Ù…Ø§Ù† ØªÙƒØ§Ù…Ù„ Ø§Ù„Ø§Ù†Ø¸Ù…Ø© ÙˆØ§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙÙŠ Ø§Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ ÙˆØªØ­Ù‚ÙŠÙ‚ Ø§Ù…Ù† ÙˆØ³Ù„Ø§Ù…Ø© Ø§Ù„Ù…Ø­ØªÙˆÙŠ Ø§Ù„Ù…Ø¹Ø±ÙÙŠ Ø§Ù„Ø§Ù„ÙƒØªØ±ÙˆÙ†ÙŠ \n",
      "\n",
      "Raw classifier output: [{'label': 'LABEL_0', 'score': 0.9728232026100159}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the classified jobs dataset\n",
    "df = pd.read_csv(\"classified_wadhefa_dataset.csv\")\n",
    "\n",
    "# Take the first job description\n",
    "sample_text = df[\"description\"].iloc[0]\n",
    "print(\"Sample job description:\\n\", sample_text, \"\\n\")\n",
    "\n",
    "# Run it through the classifier\n",
    "result = job_classifier(sample_text)\n",
    "print(\"Raw classifier output:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a046df74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved. New columns: ['category', 'description', 'url', 'predicted_category']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"classified_wadhefa_dataset.csv\")\n",
    "\n",
    "# Drop the title column if it exists\n",
    "if \"title\" in df.columns:\n",
    "    df = df.drop(columns=[\"title\"])\n",
    "\n",
    "# Save cleaned dataset\n",
    "df.to_csv(\"classified_wadhefa_dataset.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Cleaned dataset saved. New columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "26eaf267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved. Remaining rows: 3\n",
      "predicted_category\n",
      "IT           2\n",
      "Marketing    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file\n",
    "df = pd.read_csv(\"classified_wadhefa_dataset.csv\")\n",
    "\n",
    "# Keep only rows where predicted_category != \"Unknown\"\n",
    "df_clean = df[df[\"predicted_category\"] != \"Unknown\"]\n",
    "\n",
    "# Save back\n",
    "df_clean.to_csv(\"classified_wadhefa_dataset.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Cleaned dataset saved. Remaining rows:\", df_clean.shape[0])\n",
    "print(df_clean[\"predicted_category\"].value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
