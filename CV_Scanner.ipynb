{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b0b6cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upgraded to latest versions!\n"
     ]
    }
   ],
   "source": [
    "# Upgrade to latest compatible versions\n",
    "!pip install --upgrade transformers accelerate --quiet\n",
    "print(\"Upgraded to latest versions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29ef65e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compatible versions installed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [23 lines of output]\n",
      "      Checking for Rust toolchain....\n",
      "      Rust not found, installing into a temporary directory\n",
      "      Python reports SOABI: cp313-win_amd64\n",
      "      Computed rustc target triple: x86_64-pc-windows-msvc\n",
      "      Installation directory: C:\\Users\\maanx\\AppData\\Local\\puccinialin\\puccinialin\\Cache\n",
      "      Rustup already downloaded\n",
      "      Installing rust to C:\\Users\\maanx\\AppData\\Local\\puccinialin\\puccinialin\\Cache\\rustup\n",
      "      warn: It looks like you have an existing rustup settings file at:\n",
      "      warn: C:\\Users\\maanx\\.rustup\\settings.toml\n",
      "      warn: Rustup will install the default toolchain as specified in the settings file,\n",
      "      warn: instead of the one inferred from the default host triple.\n",
      "      warn: installing msvc toolchain without its prerequisites\n",
      "      info: profile set to 'minimal'\n",
      "      info: default host triple is x86_64-pc-windows-msvc\n",
      "      warn: Updating existing toolchain, profile choice will be ignored\n",
      "      info: syncing channel updates for 'stable-x86_64-pc-windows-msvc'\n",
      "      info: default toolchain set to 'stable-x86_64-pc-windows-msvc'\n",
      "      Checking if cargo is installed\n",
      "      \n",
      "      Cargo, the Rust package manager, is not installed or is not on PATH.\n",
      "      This package requires Rust and Cargo to compile extensions. Install it through\n",
      "      the system's package manager or via https://rustup.rs/\n",
      "      \n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "# Install compatible versions to avoid errors\n",
    "!pip install torch transformers==4.36.0 accelerate==0.24.1 pandas scikit-learn tqdm --quiet\n",
    "print(\"Compatible versions installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0828d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.56.1\n",
      "Accelerate version: 1.10.1\n",
      "Torch version: 2.8.0+cpu\n"
     ]
    }
   ],
   "source": [
    "# Check installed versions\n",
    "import transformers\n",
    "import accelerate\n",
    "import torch\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Accelerate version: {accelerate.__version__}\")\n",
    "print(f\"Torch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ae5a21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\maanx\\anaconda3\\lib\\site-packages (4.56.1)\n",
      "Requirement already satisfied: torch in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (2.8.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (2.3.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\maanx\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\maanx\\anaconda3\\lib\\site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\maanx\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\maanx\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\maanx\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\maanx\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch pandas numpy scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe60c4",
   "metadata": {},
   "source": [
    "Cell 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b60756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data handling and manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Deep Learning & NLP\n",
    "import torch\n",
    "from transformers import (AutoTokenizer, \n",
    "                          AutoModelForSequenceClassification, \n",
    "                          TrainingArguments, \n",
    "                          Trainer,\n",
    "                          pipeline)\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Suppress optional warnings to keep the output clean (optional)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8afb3f",
   "metadata": {},
   "source": [
    "Cell 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06d649ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Preview:\n",
      "                                                text label\n",
      "0  مطور خلفي كبير يمتلك خبرة 6 سنوات في بايثون وج...    IT\n",
      "1  مطور شامل Stack يجيد JavaScript و React.js و N...    IT\n",
      "2  مهندس DevOps لديه خبرة في خدمات السحابة AWS وا...    IT\n",
      "3  مطور واجهات أمامية متخصص في Vue.js وأطر عمل CS...    IT\n",
      "4  عالم بيانات ماهر في Python و R ومكتبات التعلم ...    IT\n",
      "\n",
      "Dataset Shape: (454, 2)\n",
      "\n",
      "Label Counts:\n",
      "label\n",
      "IT             88\n",
      "Marketing      80\n",
      "Finance        76\n",
      "Engineering    76\n",
      "Healthcare     72\n",
      "Education      62\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label Mapping: {'IT': 0, 'Marketing': 1, 'Finance': 2, 'Engineering': 3, 'Healthcare': 4, 'Education': 5}\n"
     ]
    }
   ],
   "source": [
    "# 1. Load your dataset from the CSV file\n",
    "df = pd.read_csv('fake_cvs.csv')\n",
    "\n",
    "# 2. Let's see what the data looks like\n",
    "print(\"Dataset Preview:\")\n",
    "print(df.head())\n",
    "print(f\"\\nDataset Shape: {df.shape}\") # Shows (number_of_rows, number_of_columns)\n",
    "\n",
    "# 3. Check the distribution of labels (categories)\n",
    "print(\"\\nLabel Counts:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# 4. Map text labels to numbers (e.g., \"IT\" -> 0, \"Marketing\" -> 1)\n",
    "label_list = df['label'].unique().tolist()\n",
    "label_to_id = {label: idx for idx, label in enumerate(label_list)}\n",
    "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "\n",
    "print(f\"\\nLabel Mapping: {label_to_id}\")\n",
    "\n",
    "# Apply the mapping to the DataFrame\n",
    "df['label'] = df['label'].map(label_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd378f57",
   "metadata": {},
   "source": [
    "Cell 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93063eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 408\n",
      "Number of testing examples: 46\n",
      "\n",
      "Tokenizing training texts...\n",
      "Tokenizing testing texts...\n",
      "Tokenization complete!\n"
     ]
    }
   ],
   "source": [
    "# 1. Split the data (90% for training, 10% for testing)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['text'].tolist(), \n",
    "    df['label'].tolist(), \n",
    "    test_size=0.1, \n",
    "    random_state=42 # random_state ensures you get the same split every time\n",
    ")\n",
    "\n",
    "print(f\"Number of training examples: {len(train_texts)}\")\n",
    "print(f\"Number of testing examples: {len(test_texts)}\")\n",
    "\n",
    "# 2. Load the AraBERT Tokenizer\n",
    "model_name = \"aubmindlab/bert-base-arabertv2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 3. Tokenize the texts (Convert text to numbers AraBERT understands)\n",
    "print(\"\\nTokenizing training texts...\")\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "print(\"Tokenizing testing texts...\")\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "print(\"Tokenization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af74a973",
   "metadata": {},
   "source": [
    "Cell 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c48f5f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom Dataset class\n",
    "class CVDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert tokenized inputs to tensors\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create the dataset objects\n",
    "train_dataset = CVDataset(train_encodings, train_labels)\n",
    "test_dataset = CVDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982f1f48",
   "metadata": {},
   "source": [
    "Cell 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "572a18f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model and adjust it for our number of labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_list)\n",
    ")\n",
    "\n",
    "# Print model architecture (optional)\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5238f5fc",
   "metadata": {},
   "source": [
    "Cell 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a469688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Directory to save model checkpoints\n",
    "    num_train_epochs=10,             # Number of full passes through the data\n",
    "    per_device_train_batch_size=8,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Learning rate warmup\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for logging\n",
    "    logging_steps=10,                # Log every 10 steps\n",
    "    eval_strategy=\"steps\",           # Evaluate during training (NEW PARAMETER NAME)\n",
    "    eval_steps=50,                   # Evaluate every 50 steps\n",
    "    save_steps=500,                  # Save checkpoint every 500 steps\n",
    "    load_best_model_at_end=True,     # Load the best model at the end\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bae159",
   "metadata": {},
   "source": [
    "Cell 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fe1d572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='510' max='510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [510/510 09:25, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.735900</td>\n",
       "      <td>1.717639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.491100</td>\n",
       "      <td>1.393970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.768100</td>\n",
       "      <td>0.628416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.217348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.199178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.370876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.239726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.277427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.088522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.077836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Start training!\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97832d3",
   "metadata": {},
   "source": [
    "Cell 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ca4757a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained('./my_cv_classifier')\n",
    "tokenizer.save_pretrained('./my_cv_classifier')\n",
    "print(\"Model and tokenizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c267146d",
   "metadata": {},
   "source": [
    "cell 8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "665624a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def pdf_to_text(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract raw text from a PDF file.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "    return text.strip()\n",
    "\n",
    "def extract_description(cv_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract description/summary/profile section from CV text.\n",
    "    \"\"\"\n",
    "    headers = [\n",
    "        \"summary\", \"profile\", \"professional summary\", \"career objective\",\n",
    "        \"objective\", \"about me\", \"personal profile\", \"description\",\n",
    "        \"ملخص\", \"الملف الشخصي\", \"الهدف الوظيفي\"  # Arabic headers\n",
    "    ]\n",
    "    \n",
    "    lower_text = cv_text.lower()\n",
    "    for header in headers:\n",
    "        if header in lower_text:\n",
    "            start = lower_text.index(header)\n",
    "            section = cv_text[start:]\n",
    "            stop_match = re.search(r\"(experience|education|skills|projects|خبرة|التعليم|المهارات|المشاريع)\", section, re.IGNORECASE)\n",
    "            if stop_match:\n",
    "                section = section[:stop_match.start()]\n",
    "            return section.strip()\n",
    "    \n",
    "    # fallback: first paragraph\n",
    "    first_para = cv_text.split(\"\\n\\n\")[0]\n",
    "    if len(first_para.split()) > 10:\n",
    "        return first_para.strip()\n",
    "    return cv_text\n",
    "\n",
    "def extract_description_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads a PDF and extracts only the description section.\n",
    "    \"\"\"\n",
    "    text = pdf_to_text(pdf_path)\n",
    "    return extract_description(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e51a92",
   "metadata": {},
   "source": [
    "cell 8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42548d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load trained pipeline\n",
    "classifier = pipeline('text-classification', \n",
    "                      model='./my_cv_classifier', \n",
    "                      tokenizer='./my_cv_classifier', \n",
    "                      function_to_apply='softmax')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10266eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted description:\n",
      " CAREER OBJECTIVE\n",
      "Fresh IT graduate with a strong interest in Data Science and a solid foundation in machine learning, data analysis, and\n",
      "programming. Proﬁcient in Python and key ML tools. Eager to grow in a data-driven role and apply analytical thinking to\n",
      "real-world problems. \n",
      "\n",
      "Predicted Category: IT (Confidence: 0.9983)\n"
     ]
    }
   ],
   "source": [
    "# Example: classify a PDF CV\n",
    "pdf_path = \"sample_cv.pdf\"  # change to your CV file\n",
    "desc = extract_description_from_pdf(pdf_path)\n",
    "\n",
    "print(\"Extracted description:\\n\", desc, \"\\n\")\n",
    "\n",
    "result = classifier(desc)[0]\n",
    "predicted_label = id_to_label[int(result['label'].split('_')[-1])]\n",
    "print(f\"Predicted Category: {predicted_label} (Confidence: {result['score']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab820d1",
   "metadata": {},
   "source": [
    "Cell 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6caf4475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions:\n",
      "\n",
      "Text: خبرة في تحليل البيانات واستخدام SQL و Python لاستخراج النتائج المهمة\n",
      "--> Predicted: IT (Confidence: 0.9976)\n",
      "\n",
      "Text: Experience in data analysis and using SQL and Python\n",
      "--> Predicted: IT (Confidence: 0.9980)\n",
      "\n",
      "Text: إدارة حملات التسويق على وسائل التواصل الاجتماعي\n",
      "--> Predicted: Marketing (Confidence: 0.9976)\n",
      "\n",
      "Text: Managing social media marketing campaigns\n",
      "--> Predicted: Marketing (Confidence: 0.9974)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Using the pipeline (Easiest)\n",
    "classifier = pipeline('text-classification', \n",
    "                      model=model, \n",
    "                      tokenizer=tokenizer, \n",
    "                      function_to_apply='softmax')\n",
    "\n",
    "# Test with new examples\n",
    "test_texts = [\n",
    "    \"خبرة في تحليل البيانات واستخدام SQL و Python لاستخراج النتائج المهمة\",  # Arabic IT\n",
    "    \"Experience in data analysis and using SQL and Python\",                 # English IT\n",
    "    \"إدارة حملات التسويق على وسائل التواصل الاجتماعي\",                      # Arabic Marketing\n",
    "    \"Managing social media marketing campaigns\",                            # English Marketing\n",
    "]\n",
    "\n",
    "print(\"Making predictions:\\n\")\n",
    "for text in test_texts:\n",
    "    result = classifier(text)[0]\n",
    "    predicted_label = id_to_label[int(result['label'].split('_')[-1])] # Convert output to label name\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"--> Predicted: {predicted_label} (Confidence: {result['score']:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ccaf5",
   "metadata": {},
   "source": [
    "matching function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40171528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         description  \\\n",
      "0  - تحديد مواصفات الحلول البديلة والاضافات والتح...   \n",
      "1  - Manage and optimize the company’s ecommerce ...   \n",
      "\n",
      "                                                 url predicted_category  \n",
      "0  https://www.wadhefa.com/details/job/86996/search/                 IT  \n",
      "1  https://www.wadhefa.com/details/job/86792/search/                 IT  \n"
     ]
    }
   ],
   "source": [
    "from utils_matcher import match_cv_to_jobs\n",
    "\n",
    "recommended_jobs = match_cv_to_jobs(\"IT\", jobs_csv=\"job_posts_part/classified_wadhefa_dataset.csv\", top_n=5)\n",
    "print(recommended_jobs[[\"description\", \"url\", \"predicted_category\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0670b352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (3, 4)\n",
      "Unique predicted categories: ['IT' 'Marketing']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"job_posts_part/classified_wadhefa_dataset.csv\")\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Unique predicted categories:\", df[\"predicted_category\"].unique())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
